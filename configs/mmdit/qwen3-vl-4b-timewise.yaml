trainer:
  cache_dir: "../.cache/torch_xla"
  checkpoint_dir: "/ytech_m2v8_hdd/workspace/kling_mm/libozhou/feature_combination/output/256-mmdit-layerwise"
  checkpointing_steps: 5000
  consolidation_steps: 50000
  enable_gradient_checkpointing: false
  gradient_accumulation_steps: 1
  gradient_clipping: 1.0
  hard_skip_resume: false
  logging_steps: 1
  logit_mean: 0.0
  logit_std: 1.0
  max_steps: 500_000
  mixed_precision: "bf16"
  mode_scale: 1.29
  precondition_outputs: false
  project: "AdaFuseDiT"
  resume_from: null
  run: "mmdit-timewise"
  seed: 42
  weighting_scheme: "logit_normal"
  train_dit: true
  train_llm: false

model:
  name: "MMDiT"
  base: "/ytech_m2v5_hdd/workspace/kling_mm/Models/Qwen3-VL-4B-Instruct/"
  encoder_type: "llm"
  attention: "self"
  dit_num_hidden_layers: 18
  patch_size: 2
  pos_embed: "1d-rope"
  qk_norm: true
  sandwich_norm: false
  text_hidden_states_index: -2   # use the penultimate LLM layer
  timestep_conditioning: "adaln-zero"

    # === AdaFuseDiT 特有配置 ===
  text_hidden_states_num: 36
  use_timestep_adaptive_fusion: true
  use_layer_wise_fusion: false
  adaptive_fusion_time_embed_dim: 128


ema:
  decay: 0.99
  update_steps: 100

vae:
  pretrained_model_name_or_path: "/ytech_m2v5_hdd/workspace/kling_mm/Models/Lumina-Image-2.0/"
  subfolder: "vae"

noise_scheduler:
  pretrained_model_name_or_path: "/ytech_m2v5_hdd/workspace/kling_mm/Models/Lumina-Image-2.0/scheduler/"
  subfolder: "scheduler"

optimizer:
  lr: 1e-4
  weight_decay: 1e-4

lr_scheduler:
  name: "constant"
  num_warmup_steps: 0

data:
  apply_chat_template: false
  batch_size: 8
  center_crop: true
  dataloader_num_workers: 8
  data_path: "/ytech_m2v5_hdd/workspace/kling_mm/libozhou/text_encoder/train_data/t2i_train_data_six_filtered.json"
  device_prefetch_size: 64
  instruction: "Describe this image."
  max_prompt_length: 512
  original_caption_rate: 0.0
  random_dropping_rate: 0.1
  resolution: 256
  tokenizer: "/ytech_m2v5_hdd/workspace/kling_mm/Models/Qwen3-VL-4B-Instruct/"
  persistent_workers: true
  prefetch_factor: 4