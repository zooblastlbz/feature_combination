#!/usr/bin/env python3
"""
å¯è§†åŒ– AdaFuseDiT æ¨¡å‹åœ¨ä¸åŒæ—¶é—´æ­¥å„å±‚çš„èåˆæƒé‡

ç”¨æ³•:
    python utils/visualize_fusion_weights.py \
        --checkpoint_path /path/to/checkpoint \
        --output_path fusion_weights.png \
        --timesteps 0 100 200 500 800 999 \
        --num_layers 18
"""

import argparse
import math
import os
import sys
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from matplotlib import cm
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from diffusion.models import AdaFuseDiT, TimestepWiseFeatureWeighting


def load_checkpoint(checkpoint_path):
    """åŠ è½½ checkpoint (æ”¯æŒ Accelerate/DeepSpeed æ ¼å¼)"""
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½ checkpoint: {checkpoint_path}")
    
    checkpoint_path = Path(checkpoint_path)
    
    # === æ–¹å¼ 1: Accelerate/DeepSpeed åˆ†å¸ƒå¼ checkpoint ===
    if checkpoint_path.is_dir():
        print(f"ğŸ” æ£€æµ‹åˆ°ç›®å½•ï¼Œå°è¯•åŠ è½½ Accelerate checkpoint...")
        
        # æŸ¥æ‰¾æ‰€æœ‰ model æ–‡ä»¶
        model_files = list(checkpoint_path.glob("*.bin")) + \
                     list(checkpoint_path.glob("*.pt")) + \
                     list(checkpoint_path.glob("pytorch_model*.bin"))
        
        if not model_files:
            print(f"âŒ åœ¨ç›®å½•ä¸­æœªæ‰¾åˆ° checkpoint æ–‡ä»¶")
            return None
        
        print(f"ğŸ“¦ æ‰¾åˆ° {len(model_files)} ä¸ª checkpoint åˆ†ç‰‡")
        
        # åˆå¹¶æ‰€æœ‰åˆ†ç‰‡
        merged_state_dict = {}
        for shard_file in sorted(model_files):
            print(f"   - åŠ è½½: {shard_file.name}")
            try:
                shard = torch.load(shard_file, map_location='cpu')
                
                # å¤„ç†ä¸åŒçš„å­˜å‚¨æ ¼å¼
                if isinstance(shard, dict):
                    # å¯èƒ½æ˜¯ {'model': state_dict} æˆ–ç›´æ¥æ˜¯ state_dict
                    if 'model' in shard:
                        shard = shard['model']
                    elif 'state_dict' in shard:
                        shard = shard['state_dict']
                    elif 'module' in shard:
                        shard = shard['module']
                
                # åˆå¹¶åˆ°æ€»å­—å…¸
                for key, value in shard.items():
                    # ç§»é™¤å¯èƒ½çš„ 'module.' æˆ– '_orig_mod.' å‰ç¼€
                    clean_key = key.replace('module.', '').replace('_orig_mod.', '')
                    merged_state_dict[clean_key] = value
                    
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {shard_file.name}: {e}")
                continue
        
        if merged_state_dict:
            print(f"âœ… æˆåŠŸåŠ è½½ Accelerate checkpoint ({len(merged_state_dict)} ä¸ªå‚æ•°)")
            return merged_state_dict
        else:
            print(f"âŒ æ— æ³•ä»ç›®å½•åŠ è½½ä»»ä½•å‚æ•°")
            return None
    
    # === æ–¹å¼ 2: å•æ–‡ä»¶ checkpoint ===
    else:
        # å°è¯•ä¸åŒçš„åŠ è½½æ–¹å¼
        try:
            # æ–¹å¼2.1: ç›´æ¥åŠ è½½
            state_dict = torch.load(checkpoint_path, map_location='cpu')
            
            # å¤„ç†å¯èƒ½çš„åµŒå¥—ç»“æ„
            if isinstance(state_dict, dict):
                if 'model' in state_dict:
                    state_dict = state_dict['model']
                elif 'state_dict' in state_dict:
                    state_dict = state_dict['state_dict']
                elif 'module' in state_dict:
                    state_dict = state_dict['module']
            
            # æ¸…ç† key å‰ç¼€
            cleaned_state_dict = {}
            for key, value in state_dict.items():
                clean_key = key.replace('module.', '').replace('_orig_mod.', '')
                cleaned_state_dict[clean_key] = value
            
            print(f"âœ… æˆåŠŸåŠ è½½ checkpoint (ç›´æ¥æ¨¡å¼, {len(cleaned_state_dict)} ä¸ªå‚æ•°)")
            return cleaned_state_dict
            
        except Exception as e:
            print(f"âš ï¸ ç›´æ¥åŠ è½½å¤±è´¥: {e}")
            
            # æ–¹å¼2.2: å°è¯•åŠ è½½å‹ç¼©çš„ checkpoint
            try:
                import zstandard as zstd
                with open(checkpoint_path, 'rb') as f:
                    dctx = zstd.ZstdDecompressor()
                    decompressed = dctx.decompress(f.read())
                    import io
                    state_dict = torch.load(io.BytesIO(decompressed), map_location='cpu')
                
                # å¤„ç†å¯èƒ½çš„åµŒå¥—ç»“æ„
                if isinstance(state_dict, dict):
                    if 'model' in state_dict:
                        state_dict = state_dict['model']
                    elif 'state_dict' in state_dict:
                        state_dict = state_dict['state_dict']
                
                # æ¸…ç† key å‰ç¼€
                cleaned_state_dict = {}
                for key, value in state_dict.items():
                    clean_key = key.replace('module.', '').replace('_orig_mod.', '')
                    cleaned_state_dict[clean_key] = value
                
                print(f"âœ… æˆåŠŸåŠ è½½ checkpoint (å‹ç¼©æ¨¡å¼, {len(cleaned_state_dict)} ä¸ªå‚æ•°)")
                return cleaned_state_dict
                
            except Exception as e2:
                print(f"âŒ åŠ è½½å¤±è´¥: {e2}")
                return None


def extract_fusion_weights(checkpoint_path, timesteps, num_dit_layers):
    """
    ä» checkpoint ä¸­æå–èåˆæƒé‡
    
    Args:
        checkpoint_path: checkpoint æ–‡ä»¶è·¯å¾„
        timesteps: è¦åˆ†æçš„æ—¶é—´æ­¥åˆ—è¡¨
        num_dit_layers: DiT å±‚æ•°
    
    Returns:
        weights_dict: {
            'global': np.array (num_timesteps, num_text_layers) æˆ– None,
            'layer_wise': np.array (num_timesteps, num_dit_layers, num_text_layers) æˆ– None,
            'config': dict
        }
    """
    state_dict = load_checkpoint(checkpoint_path)
    if state_dict is None:
        return None
    
    # æ£€æµ‹æ˜¯å“ªç§èåˆæ¨¡å¼
    has_global_module = any('text_fusion_module' in k for k in state_dict.keys())
    has_layer_wise_modules = any('text_fusion_modules' in k for k in state_dict.keys())
    has_global_weight = any('text_fusion_weight' in k for k in state_dict.keys())
    has_layer_wise_weights = any('text_fusion_weights' in k for k in state_dict.keys())
    
    # åˆ¤æ–­æ¨¡å¼
    use_timestep_adaptive = has_global_module or has_layer_wise_modules
    use_layer_wise = has_layer_wise_modules or has_layer_wise_weights
    
    print(f"\nğŸ“Š æ£€æµ‹åˆ°çš„èåˆæ¨¡å¼:")
    print(f"   - use_timestep_adaptive_fusion: {use_timestep_adaptive}")
    print(f"   - use_layer_wise_fusion: {use_layer_wise}")
    
    # æ£€æµ‹æ–‡æœ¬å±‚æ•°
    if has_global_module:
        # ä» weight_generator çš„è¾“å‡ºå±‚æ¨æ–­
        for key in state_dict.keys():
            if 'text_fusion_module.weight_generator.2.weight' in key:
                num_text_layers = state_dict[key].shape[0]
                break
    elif has_layer_wise_modules:
        for key in state_dict.keys():
            if 'text_fusion_modules.0.weight_generator.2.weight' in key:
                num_text_layers = state_dict[key].shape[0]
                break
    elif has_global_weight:
        num_text_layers = state_dict['text_fusion_weight'].shape[0]
    elif has_layer_wise_weights:
        num_text_layers = state_dict['text_fusion_weights.0'].shape[0]
    else:
        print("âŒ æ— æ³•æ£€æµ‹æ–‡æœ¬å±‚æ•°")
        return None
    
    print(f"   - text_hidden_states_num: {num_text_layers}")
    print(f"   - dit_num_hidden_layers: {num_dit_layers}")
    
    results = {
        'config': {
            'use_timestep_adaptive': use_timestep_adaptive,
            'use_layer_wise': use_layer_wise,
            'num_text_layers': num_text_layers,
            'num_dit_layers': num_dit_layers,
        }
    }
    
    # === æå–æƒé‡ ===
    if use_timestep_adaptive:
        # æ¨¡å¼ 2 æˆ– 4: æ—¶é—´è‡ªé€‚åº”èåˆ
        print(f"\nğŸ”„ è®¡ç®—æ—¶é—´è‡ªé€‚åº”æƒé‡ (å…± {len(timesteps)} ä¸ªæ—¶é—´æ­¥)...")
        
        if use_layer_wise:
            # æ¨¡å¼ 4: æ¯å±‚ç‹¬ç«‹çš„æ—¶é—´è‡ªé€‚åº”
            weights = np.zeros((len(timesteps), num_dit_layers, num_text_layers))
            
            for dit_layer_idx in tqdm(range(num_dit_layers), desc="DiTå±‚"):
                # é‡å»º TimestepWiseFeatureWeighting æ¨¡å—
                module_state = {}
                prefix = f'text_fusion_modules.{dit_layer_idx}.'
                
                for key in state_dict.keys():
                    if key.startswith(prefix):
                        new_key = key[len(prefix):]
                        module_state[new_key] = state_dict[key]
                
                # æ¨æ–­ time_embed_dim
                time_embed_dim = module_state['weight_generator.0.weight'].shape[1]
                
                # åˆ›å»ºä¸´æ—¶æ¨¡å—
                temp_module = TimestepWiseFeatureWeighting(
                    num_layers=num_text_layers,
                    time_embed_dim=time_embed_dim,
                    feature_dim=2048  # å‡è®¾å€¼ï¼Œä¸å½±å“æƒé‡è®¡ç®—
                )
                temp_module.load_state_dict(module_state)
                temp_module.eval()
                
                # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„æƒé‡
                for t_idx, t in enumerate(timesteps):
                    normalized_t = torch.tensor([t / 1000.0], dtype=torch.float32)
                    t_embed = temp_module._time_embedding(normalized_t)
                    weight = temp_module.weight_generator(t_embed)
                    weights[t_idx, dit_layer_idx, :] = weight.detach().numpy()[0]
            
            results['layer_wise'] = weights
            results['global'] = None
            
        else:
            # æ¨¡å¼ 2: å…¨å±€æ—¶é—´è‡ªé€‚åº”
            weights = np.zeros((len(timesteps), num_text_layers))
            
            # é‡å»º TimestepWiseFeatureWeighting æ¨¡å—
            module_state = {}
            prefix = 'text_fusion_module.'
            
            for key in state_dict.keys():
                if key.startswith(prefix):
                    new_key = key[len(prefix):]
                    module_state[new_key] = state_dict[key]
            
            # æ¨æ–­ time_embed_dim
            time_embed_dim = module_state['weight_generator.0.weight'].shape[1]
            
            # åˆ›å»ºä¸´æ—¶æ¨¡å—
            temp_module = TimestepWiseFeatureWeighting(
                num_layers=num_text_layers,
                time_embed_dim=time_embed_dim,
                feature_dim=2048  # å‡è®¾å€¼
            )
            temp_module.load_state_dict(module_state)
            temp_module.eval()
            
            # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„æƒé‡
            for t_idx, t in tqdm(enumerate(timesteps), total=len(timesteps), desc="æ—¶é—´æ­¥"):
                normalized_t = torch.tensor([t / 1000.0], dtype=torch.float32)
                t_embed = temp_module._time_embedding(normalized_t)
                weight = temp_module.weight_generator(t_embed)
                weights[t_idx, :] = weight.detach().numpy()[0]
            
            results['global'] = weights
            results['layer_wise'] = None
    
    else:
        # æ¨¡å¼ 1 æˆ– 3: å›ºå®šæƒé‡ï¼ˆä¸ä¾èµ–æ—¶é—´æ­¥ï¼‰
        print(f"\nğŸ“Œ æå–å›ºå®šæƒé‡...")
        
        if use_layer_wise:
            # æ¨¡å¼ 3: æ¯å±‚ç‹¬ç«‹çš„å›ºå®šæƒé‡
            weights = np.zeros((num_dit_layers, num_text_layers))
            
            for dit_layer_idx in range(num_dit_layers):
                raw_weight = state_dict[f'text_fusion_weights.{dit_layer_idx}']
                weight = F.softmax(raw_weight, dim=0).detach().numpy()
                weights[dit_layer_idx, :] = weight
            
            # å¯¹äºå›ºå®šæƒé‡ï¼Œæ‰€æœ‰æ—¶é—´æ­¥éƒ½ç›¸åŒ
            results['layer_wise'] = np.repeat(weights[np.newaxis, :, :], len(timesteps), axis=0)
            results['global'] = None
            
        else:
            # æ¨¡å¼ 1: å…¨å±€å›ºå®šæƒé‡
            raw_weight = state_dict['text_fusion_weight']
            weight = F.softmax(raw_weight, dim=0).detach().numpy()
            
            # å¯¹äºå›ºå®šæƒé‡ï¼Œæ‰€æœ‰æ—¶é—´æ­¥éƒ½ç›¸åŒ
            results['global'] = np.repeat(weight[np.newaxis, :], len(timesteps), axis=0)
            results['layer_wise'] = None
    
    return results


def plot_fusion_weights(weights_dict, timesteps, output_path, dpi=300):
    """
    ç»˜åˆ¶èåˆæƒé‡å¯è§†åŒ–å›¾
    
    Args:
        weights_dict: extract_fusion_weights è¿”å›çš„å­—å…¸
        timesteps: æ—¶é—´æ­¥åˆ—è¡¨
        output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„
        dpi: å›¾ç‰‡åˆ†è¾¨ç‡
    """
    config = weights_dict['config']
    num_text_layers = config['num_text_layers']
    use_layer_wise = config['use_layer_wise']
    
    if use_layer_wise:
        # æ¯å±‚ç‹¬ç«‹èåˆï¼šä¸ºæ¯ä¸ª DiT å±‚ç”»ä¸€ä¸ªå­å›¾
        weights = weights_dict['layer_wise']  # (num_timesteps, num_dit_layers, num_text_layers)
        num_dit_layers = weights.shape[1]
        
        # è®¡ç®—å­å›¾å¸ƒå±€
        num_cols = min(3, num_dit_layers)
        num_rows = math.ceil(num_dit_layers / num_cols)
        
        fig, axes = plt.subplots(num_rows, num_cols, figsize=(6*num_cols, 4*num_rows))
        if num_dit_layers == 1:
            axes = np.array([axes])
        axes = axes.flatten()
        
        # ä½¿ç”¨ä¸åŒçš„é¢œè‰²å’Œçº¿å‹
        colors = plt.cm.viridis(np.linspace(0, 1, len(timesteps)))
        linestyles = ['-', '--', '-.', ':'] * ((len(timesteps) // 4) + 1)
        
        for dit_layer_idx in range(num_dit_layers):
            ax = axes[dit_layer_idx]
            
            for t_idx, t in enumerate(timesteps):
                layer_weights = weights[t_idx, dit_layer_idx, :]
                ax.plot(
                    range(num_text_layers),
                    layer_weights,
                    marker='o',
                    linestyle=linestyles[t_idx],
                    color=colors[t_idx],
                    linewidth=2,
                    markersize=6,
                    label=f't={t}',
                    alpha=0.8
                )
            
            ax.set_xlabel('Text Layer Index', fontsize=12, fontweight='bold')
            ax.set_ylabel('Weight (Softmax)', fontsize=12, fontweight='bold')
            ax.set_title(f'DiT Layer {dit_layer_idx}', fontsize=14, fontweight='bold')
            ax.set_xticks(range(num_text_layers))
            ax.set_xticklabels([f'L{i}' for i in range(num_text_layers)])
            ax.grid(True, alpha=0.3, linestyle='--')
            ax.legend(loc='best', fontsize=10, ncol=2)
            ax.set_ylim([0, 1])
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(num_dit_layers, len(axes)):
            axes[idx].set_visible(False)
        
        plt.suptitle(
            f'Layer-wise Fusion Weights across Timesteps\n'
            f'({"Adaptive" if config["use_timestep_adaptive"] else "Fixed"})',
            fontsize=16,
            fontweight='bold',
            y=0.995
        )
        
    else:
        # å…¨å±€å…±äº«èåˆï¼šç”»ä¸€ä¸ªå›¾
        weights = weights_dict['global']  # (num_timesteps, num_text_layers)
        
        fig, ax = plt.subplots(figsize=(12, 7))
        
        # ä½¿ç”¨ä¸åŒçš„é¢œè‰²å’Œçº¿å‹
        colors = plt.cm.viridis(np.linspace(0, 1, len(timesteps)))
        linestyles = ['-', '--', '-.', ':'] * ((len(timesteps) // 4) + 1)
        markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p'] * ((len(timesteps) // 8) + 1)
        
        for t_idx, t in enumerate(timesteps):
            layer_weights = weights[t_idx, :]
            ax.plot(
                range(num_text_layers),
                layer_weights,
                marker=markers[t_idx],
                linestyle=linestyles[t_idx],
                color=colors[t_idx],
                linewidth=2.5,
                markersize=8,
                label=f'Timestep {t}',
                alpha=0.85
            )
        
        ax.set_xlabel('Text Layer Index', fontsize=14, fontweight='bold')
        ax.set_ylabel('Fusion Weight (After Softmax)', fontsize=14, fontweight='bold')
        ax.set_title(
            f'Global Fusion Weights across Timesteps\n'
            f'({"Timestep-Adaptive" if config["use_timestep_adaptive"] else "Fixed"})',
            fontsize=16,
            fontweight='bold',
            pad=20
        )
        ax.set_xticks(range(num_text_layers))
        ax.set_xticklabels([f'Layer {i}' for i in range(num_text_layers)])
        ax.grid(True, alpha=0.3, linestyle='--', linewidth=1)
        ax.legend(loc='best', fontsize=12, ncol=2, framealpha=0.9)
        ax.set_ylim([0, 1.0])
        
        # æ·»åŠ æ°´å¹³å‚è€ƒçº¿
        ax.axhline(y=1.0/num_text_layers, color='red', linestyle=':', linewidth=1.5, alpha=0.5, label='Uniform')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=dpi, bbox_inches='tight')
    print(f"\nâœ… å¯è§†åŒ–å›¾å·²ä¿å­˜åˆ°: {output_path}")
    
    return fig


def main():
    parser = argparse.ArgumentParser(
        description='å¯è§†åŒ– AdaFuseDiT æ¨¡å‹çš„èåˆæƒé‡'
    )
    parser.add_argument(
        '--checkpoint_path',
        type=str,
        default='/ytech_m2v5_hdd/workspace/kling_mm/libozhou/feature_combination/output/256-AdaFuseDiT-timewise/25000/mp_rank_00_model_states.pt',
        help='Checkpoint æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: checkpoints/model/ema.pt.zst æˆ– model.pt)'
    )
    parser.add_argument(
        '--output_path',
        type=str,
        default='visual/fusion_weights_visualization.png',
        help='è¾“å‡ºå›¾ç‰‡è·¯å¾„'
    )
    parser.add_argument(
        '--timesteps',
        type=int,
        nargs='+',
        default=[0, 100, 200, 300, 500, 700, 900, 999],
        help='è¦åˆ†æçš„æ—¶é—´æ­¥åˆ—è¡¨ (ç©ºæ ¼åˆ†éš”ï¼Œé»˜è®¤: 0 100 200 300 500 700 900 999)'
    )
    parser.add_argument(
        '--num_layers',
        type=int,
        default=18,
        help='DiT å±‚æ•° (é»˜è®¤: 18)'
    )
    parser.add_argument(
        '--dpi',
        type=int,
        default=300,
        help='å›¾ç‰‡åˆ†è¾¨ç‡ (é»˜è®¤: 300)'
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¨ AdaFuseDiT èåˆæƒé‡å¯è§†åŒ–å·¥å…·")
    print("=" * 60)
    print(f"ğŸ“ Checkpoint: {args.checkpoint_path}")
    print(f"ğŸ–¼ï¸  è¾“å‡ºè·¯å¾„: {args.output_path}")
    print(f"â±ï¸  æ—¶é—´æ­¥: {args.timesteps}")
    print(f"ğŸ”¢ DiT å±‚æ•°: {args.num_layers}")
    print("=" * 60)
    
    # æå–æƒé‡
    weights_dict = extract_fusion_weights(
        args.checkpoint_path,
        args.timesteps,
        args.num_layers
    )
    
    if weights_dict is None:
        print("\nâŒ æƒé‡æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ checkpoint è·¯å¾„")
        return
    
    # ç»˜åˆ¶å¯è§†åŒ–
    plot_fusion_weights(
        weights_dict,
        args.timesteps,
        args.output_path,
        dpi=args.dpi
    )
    
    print("\n" + "=" * 60)
    print("âœ¨ å®Œæˆï¼")
    print("=" * 60)


if __name__ == '__main__':
    main()
