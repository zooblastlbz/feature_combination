from functools import partial
import random
import json
from pathlib import Path

from datasets import load_dataset
from diffusers.utils import is_torch_xla_available
import numpy as np
from PIL import Image, ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
import torch
from torch.utils.data import Dataset
from torchvision import transforms
from transformers import CLIPTokenizer, GemmaTokenizer
import webdataset as wds


class LocalImageTextDataset(Dataset):
    """
    æœ¬åœ° JSON æ ¼å¼æ•°æ®é›†åŠ è½½å™¨ï¼ˆä½¿ç”¨ HuggingFace datasetsï¼‰
    
    æ”¯æŒçš„ JSON æ ¼å¼:
    [
        {"image": "path/to/image1.jpg", "text": "caption for image 1"},
        {"image": "path/to/image2.jpg", "text": "caption for image 2"},
        ...
    ]
    
    å¯ä»¥é€šè¿‡ hparams.data.image_key å’Œ hparams.data.text_key è‡ªå®šä¹‰é”®å
    
    ä½¿ç”¨ datasets åº“çš„ä¼˜åŠ¿ï¼š
    - è‡ªåŠ¨ç¼“å­˜å’Œå†…å­˜æ˜ å°„
    - æ”¯æŒæµå¼åŠ è½½å¤§å‹æ•°æ®é›†
    - æ›´å¥½çš„æ€§èƒ½å’Œå†…å­˜ç®¡ç†
    """
    def __init__(self, json_path, hparams, tokenizer, image_root=None):
        """
        Args:
            json_path: JSON æ–‡ä»¶è·¯å¾„
            hparams: è®­ç»ƒè¶…å‚æ•°
            tokenizer: æ–‡æœ¬åˆ†è¯å™¨
            image_root: å›¾åƒæ ¹ç›®å½•ï¼ˆå¦‚æœ JSON ä¸­çš„è·¯å¾„æ˜¯ç›¸å¯¹è·¯å¾„ï¼‰
        """
        self.hparams = hparams
        self.tokenizer = tokenizer
        self.image_root = Path(image_root) if image_root else None
        
        # ä»é…ç½®ä¸­è·å–é”®åï¼Œæä¾›é»˜è®¤å€¼
        self.image_key = getattr(hparams.data, 'image_key', 'image')
        self.text_key = getattr(hparams.data, 'text_key', 'text')
        
        # ä½¿ç”¨ datasets åŠ è½½ JSON æ•°æ®
        print(f"ğŸ“š æ­£åœ¨ä½¿ç”¨ datasets åº“åŠ è½½ {json_path}...")
        self.dataset = load_dataset('json', data_files=json_path, split='train')
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.dataset)} ä¸ªæ ·æœ¬ä» {json_path}")
        print(f"ğŸ“Œ ä½¿ç”¨é”®å: image_key='{self.image_key}', text_key='{self.text_key}'")
        print(f"ğŸ“Š æ•°æ®é›†ç‰¹å¾: {self.dataset.column_names}")
        
        # éªŒè¯æ•°æ®é›†æ˜¯å¦åŒ…å«æŒ‡å®šçš„é”®
        if self.image_key not in self.dataset.column_names:
            raise KeyError(
                f"âŒ JSON ä¸­æ‰¾ä¸åˆ°å›¾åƒé”® '{self.image_key}'ï¼Œ"
                f"å¯ç”¨çš„é”®: {self.dataset.column_names}"
            )
        if self.text_key not in self.dataset.column_names:
            raise KeyError(
                f"âŒ JSON ä¸­æ‰¾ä¸åˆ°æ–‡æœ¬é”® '{self.text_key}'ï¼Œ"
                f"å¯ç”¨çš„é”®: {self.dataset.column_names}"
            )
        
        # å›¾åƒé¢„å¤„ç†å˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize(hparams.data.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
            (transforms.CenterCrop(hparams.data.resolution) if hparams.data.center_crop 
             else transforms.RandomCrop(hparams.data.resolution)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ])
    
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, idx):
        # ä½¿ç”¨ datasets è·å–æ ·æœ¬
        item = self.dataset[idx]
        
        # ä½¿ç”¨é…ç½®çš„é”®ååŠ è½½å›¾åƒ
        image_path = item[self.image_key]
        if self.image_root:
            image_path = self.image_root / image_path
        
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½å›¾åƒ {image_path}: {e}")
            # è¿”å›ä¸€ä¸ªçº¯é»‘å›¾åƒä½œä¸ºå ä½ç¬¦
            image = Image.new('RGB', (self.hparams.data.resolution, self.hparams.data.resolution), (0, 0, 0))
        
        # åº”ç”¨å›¾åƒå˜æ¢
        pixel_values = self.transform(image)
        
        # ä½¿ç”¨é…ç½®çš„é”®åè·å–æ–‡æœ¬
        caption = str(item[self.text_key])  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
        
        # éšæœºä¸¢å¼ƒ captionï¼ˆç”¨äº CFG è®­ç»ƒï¼‰
        if random.random() < self.hparams.data.random_dropping_rate:
            caption = ""
        else:
            # æ·»åŠ  instruction å‰ç¼€
            instruction = getattr(self.hparams.data, 'instruction', '')
            caption = instruction + caption
        
        # Tokenize æ–‡æœ¬
        if hasattr(self.hparams.data, 'apply_chat_template') and self.hparams.data.apply_chat_template and caption != "":
            tokenized = self.tokenizer.apply_chat_template(
                [{"role": "user", "content": caption}],
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=self.hparams.data.max_prompt_length + self.hparams.data.instruction_length,
                add_generation_prompt=self.hparams.data.add_generation_prompt,
                return_dict=True,
            )
            input_ids = tokenized["input_ids"]
            attention_mask = tokenized["attention_mask"]
        else:
            tokenized = self.tokenizer(
                caption,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=self.hparams.data.max_prompt_length + self.hparams.data.instruction_length,
            )
            input_ids = tokenized.input_ids
            attention_mask = tokenized.attention_mask
        
        return {
            "pixel_values": pixel_values,
            "input_ids": input_ids,
            "attention_mask": attention_mask,
        }


def get_local_json_dataloader(hparams, *args, **kwargs):
    """
    åˆ›å»ºæœ¬åœ° JSON æ•°æ®é›†çš„ DataLoader
    
    åœ¨ YAML é…ç½®æ–‡ä»¶ä¸­æ·»åŠ :
    data:
      data_path: "path/to/your/data.json"  # JSON æ–‡ä»¶è·¯å¾„
      image_root: "path/to/images"          # å¯é€‰ï¼Œå›¾åƒæ ¹ç›®å½•
      use_local_json: true                  # å¯ç”¨æœ¬åœ° JSON åŠ è½½
    """
    tokenizer = GemmaTokenizer.from_pretrained(hparams.data.tokenizer)
    
    # è®¡ç®— instruction é•¿åº¦
    if hasattr(hparams.data, 'apply_chat_template') and hparams.data.apply_chat_template:
        hparams.data.instruction_length = tokenizer.apply_chat_template(
            [{"role": "user", "content": hparams.data.instruction.rstrip()}],
            return_tensors="pt",
            padding=False,
            truncation=True,
            max_length=hparams.data.max_prompt_length,
            add_generation_prompt=hparams.data.add_generation_prompt,
            return_dict=True,
        )["input_ids"].shape[1] - 1
    else:
        hparams.data.instruction_length = tokenizer(
            hparams.data.instruction.rstrip(),
            return_tensors="pt",
            padding=False,
            truncation=True,
            max_length=hparams.data.max_prompt_length,
        ).input_ids.shape[1] - 1
    
    # åˆ›å»ºæ•°æ®é›†
    image_root = hparams.data.image_root if hasattr(hparams.data, 'image_root') else None
    dataset = LocalImageTextDataset(
        json_path=hparams.data.data_path,
        hparams=hparams,
        tokenizer=tokenizer,
        image_root=image_root
    )
    
    def seed_worker(worker_id):
        worker_seed = torch.initial_seed() % (2 ** 32)
        random.seed(worker_seed)
        np.random.seed(worker_seed)
    
    return torch.utils.data.DataLoader(
        dataset,
        batch_size=hparams.data.batch_size,
        shuffle=True,  # æœ¬åœ°æ•°æ®é›†éœ€è¦æ‰‹åŠ¨ shuffle
        collate_fn=llm_collate_fn,
        num_workers=hparams.data.dataloader_num_workers,
        generator=torch.manual_seed(hparams.trainer.seed),
        worker_init_fn=seed_worker,
        pin_memory=False if is_torch_xla_available() else True,
        drop_last=True,  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„ batch
    )


def llm_preprocess_fn(hparams, tokenizer, sample):
    image = sample[hparams.data.image_column]

    if hparams.data.original_caption_rate > 0 and sample.get(hparams.data.caption_column.original) is not None and random.random() < hparams.data.original_caption_rate:
        caption = sample[hparams.data.caption_column.original]
    else:
        caption = sample[hparams.data.caption_column.synthetic]

    transform = transforms.Compose(
        [
            transforms.Resize(hparams.data.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
            (transforms.CenterCrop(hparams.data.resolution) if hparams.data.center_crop else transforms.RandomCrop(hparams.data.resolution)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ]
    )
    pixel_values = transform(image)

    if random.random() < hparams.data.random_dropping_rate: # Randomly drop the caption
        caption = ""
    else:
        caption = hparams.data.instruction + caption

    if hparams.data.apply_chat_template and caption != "":
        tokenized = tokenizer.apply_chat_template(
            [{ "role": "user", "content": caption }],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=hparams.data.max_prompt_length + hparams.data.instruction_length,
            add_generation_prompt=hparams.data.add_generation_prompt,
            return_dict=True,
        )
        input_ids = tokenized["input_ids"]
        attention_mask = tokenized["attention_mask"]
    else:
        tokenized = tokenizer(
            caption,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=hparams.data.max_prompt_length + hparams.data.instruction_length,
        )
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask

    return {
        "pixel_values": pixel_values,
        "input_ids": input_ids,
        "attention_mask": attention_mask,
    }


def llm_collate_fn(examples):
    pixel_values = [example["pixel_values"] for example in examples]
    pixel_values = torch.stack(pixel_values).to(memory_format=torch.contiguous_format).float()
    input_ids = [example["input_ids"] for example in examples]
    input_ids = torch.cat(input_ids).to(memory_format=torch.contiguous_format)
    attention_mask = [example["attention_mask"] for example in examples]
    attention_mask = torch.cat(attention_mask).to(memory_format=torch.contiguous_format)

    return {
        "pixel_values": pixel_values,
        "input_ids": input_ids,
        "attention_mask": attention_mask
    }


def get_llm_dataloader(hparams, *args, **kwargs):
    tokenizer = GemmaTokenizer.from_pretrained(hparams.data.tokenizer)

    if hparams.data.apply_chat_template:
        hparams.data.instruction_length = tokenizer.apply_chat_template(
            [{ "role": "user", "content": hparams.data.instruction.rstrip() }],
            return_tensors="pt",
            padding=False,
            truncation=True,
            max_length=hparams.data.max_prompt_length,
            add_generation_prompt=hparams.data.add_generation_prompt,
            return_dict=True,
        )["input_ids"].shape[1] - 1
    else:
        hparams.data.instruction_length = tokenizer(
            hparams.data.instruction.rstrip(),
            return_tensors="pt",
            padding=False,
            truncation=True,
            max_length=hparams.data.max_prompt_length,
        ).input_ids.shape[1] - 1

    dataset = (
        wds.WebDataset(wds.ResampledShards(hparams.data.data_path, deterministic=True))
            .shuffle(1000, rng=random.Random(hparams.trainer.seed))
            .decode("pil")
            .map(
                partial(
                    llm_preprocess_fn,
                    hparams,
                    tokenizer,
                ),
            )
    )

    def seed_worker(worker_id):
        worker_seed = torch.initial_seed() % (2 ** 32)
        random.seed(worker_seed)
        np.random.seed(worker_seed)

    return torch.utils.data.DataLoader(
        dataset,
        batch_size=hparams.data.batch_size,
        collate_fn=llm_collate_fn,
        num_workers=hparams.data.dataloader_num_workers,
        generator=torch.manual_seed(hparams.trainer.seed),
        worker_init_fn=seed_worker,
        pin_memory=False if is_torch_xla_available() else True,
        prefetch_factor=8,
    )


def clip_llm_preprocess_fn(hparams, clip_tokenizer, tokenizer, sample):
    image = sample[hparams.data.image_column]

    if hparams.data.original_caption_rate > 0 and sample.get(hparams.data.caption_column.original) is not None and random.random() < hparams.data.original_caption_rate:
        caption = sample[hparams.data.caption_column.original]
    else:
        caption = sample[hparams.data.caption_column.synthetic]

    transform = transforms.Compose(
        [
            transforms.Resize(hparams.data.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
            (transforms.CenterCrop(hparams.data.resolution) if hparams.data.center_crop else transforms.RandomCrop(hparams.data.resolution)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ]
    )
    pixel_values = transform(image)

    if random.random() < hparams.data.random_dropping_rate: # Randomly drop the caption
        caption = ""
    else:
        caption = hparams.data.instruction + caption

    clip_input_ids = clip_tokenizer(
        caption,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=77,
    ).input_ids

    if hparams.data.apply_chat_template and caption != "":
        tokenized = tokenizer.apply_chat_template(
            [{ "role": "user", "content": caption }],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=hparams.data.max_prompt_length + hparams.data.instruction_length,
            add_generation_prompt=hparams.data.add_generation_prompt,
            return_dict=True,
        )
        input_ids = tokenized["input_ids"]
        attention_mask = tokenized["attention_mask"]
    else:
        tokenized = tokenizer(
            caption,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=hparams.data.max_prompt_length + hparams.data.instruction_length,
        )
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask

    return {
        "pixel_values": pixel_values,
        "clip_input_ids": clip_input_ids,
        "input_ids": input_ids,
        "attention_mask": attention_mask,
    }


def clip_llm_collate_fn(examples):
    pixel_values = [example["pixel_values"] for example in examples]
    pixel_values = torch.stack(pixel_values).to(memory_format=torch.contiguous_format).float()
    clip_input_ids = [example["clip_input_ids"] for example in examples]
    clip_input_ids = torch.cat(clip_input_ids).to(memory_format=torch.contiguous_format)
    input_ids = [example["input_ids"] for example in examples]
    input_ids = torch.cat(input_ids).to(memory_format=torch.contiguous_format)
    attention_mask = [example["attention_mask"] for example in examples]
    attention_mask = torch.cat(attention_mask).to(memory_format=torch.contiguous_format)

    return {
        "pixel_values": pixel_values,
        "clip_input_ids": clip_input_ids,
        "input_ids": input_ids,
        "attention_mask": attention_mask
    }


def get_clip_llm_dataloader(hparams, *args, **kwargs):
    clip_tokenizer = CLIPTokenizer.from_pretrained(**hparams.data.tokenizer.clip)
    tokenizer = GemmaTokenizer.from_pretrained(hparams.data.tokenizer.llm)

    hparams.data.instruction_length = tokenizer(
        hparams.data.instruction.rstrip(),
        return_tensors="pt",
        padding=False,
        truncation=True,
        max_length=hparams.data.max_prompt_length,
    ).input_ids.shape[1] - 1

    dataset = (
        wds.WebDataset(wds.ResampledShards(hparams.data.data_path, deterministic=True))
            .shuffle(1000, rng=random.Random(hparams.trainer.seed))
            .decode("pil")
            .map(
                partial(
                    clip_llm_preprocess_fn,
                    hparams,
                    clip_tokenizer,
                    tokenizer,
                ),
            )
    )

    def seed_worker(worker_id):
        worker_seed = torch.initial_seed() % (2 ** 32)
        random.seed(worker_seed)
        np.random.seed(worker_seed)

    return torch.utils.data.DataLoader(
        dataset,
        batch_size=hparams.data.batch_size,
        collate_fn=clip_llm_collate_fn,
        num_workers=hparams.data.dataloader_num_workers,
        generator=torch.manual_seed(hparams.trainer.seed),
        worker_init_fn=seed_worker,
        pin_memory=False if is_torch_xla_available() else True,
        prefetch_factor=8,
    )


def get_dataloader(hparams, *args, **kwargs):
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æœ¬åœ° JSON æ•°æ®é›†
    if hasattr(hparams.data, 'use_local_json') and hparams.data.use_local_json:
        return get_local_json_dataloader(hparams, *args, **kwargs)
    
    if hparams.model.encoder_type == "clip-llm":
        return get_clip_llm_dataloader(hparams, *args, **kwargs)
    elif hparams.model.encoder_type == "llm":
        return get_llm_dataloader(hparams, *args, **kwargs)
    else:
        raise ValueError(f"Invalid encoder_type: {hparams.model.encoder_type}")